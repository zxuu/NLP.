{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 1、TF-IDF算法介绍\n",
    "       TF-IDF（term frequency–inverse document frequency，词频-逆向文件频率）是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。\n",
    "TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。\n",
    "TF-IDF的主要思想是：如果某个单词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。\n",
    "上述引用总结就是, 一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.\n",
    "2.TF\n",
    "词频 (term frequency, TF) 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。）\n",
    "但是, 需要注意, 一些通用的词语对于主题并没有太大的作用, 反倒是一些出现频率较少的词才能够表达文章的主题, 所以单纯使用是TF不合适的。\n",
    "权重的设计必须满足：一个词预测主题的能力越强，权重越大，反之，权重越小。所有统计的文章中，一些词只是在其中很少几篇文章中出现，那么这样的词对文章的主题的作用很大，这些词的权重应该设计的较大。IDF就是在完成这样的工作.\n",
    "TF计算公式：\n",
    "\n",
    "3.IDF\n",
    "\n",
    "逆向文件频率 (IDF) ：某一特定词语的IDF，可以由总文件数目除以包含该词语的文件的数目，再将得到的商取对数得到。\n",
    "如果包含词条t的文档越少, IDF越大，则说明词条具有很好的类别区分能力。\n",
    "IDF计算公式：\n",
    "　某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。\n",
    "TF-IDF就是TF与IDF的乘积：\n",
    "  tf-idf = tf * idf\n",
    "tf = 指定词语在该文档中出现次数/指定文档中的所有词语数\n",
    "idf = log10(所有文档数/指定词语所出现的文档数)\n",
    "如上公式，总结起来就是，词语的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 示例\n",
    "假如一篇文档的总词语数是 100 个，其中“玻尿酸”这个词出现了 6 次，那“玻尿酸”一词在该文档中的词频（TF）就是 6/100 = 0.06。而计算文档\n",
    "频率（IDF）的方法是以文档集的文档总数，除以出现“玻尿酸”一词的文档数。所以，如果“玻尿酸”一词在 100 份文件出现过，而文件总数是 1000000\n",
    "份的话，其逆文档频率（IDF）就是 log10(1000000/100) = 4。最后 tf-idf 的分数为 0.06 * 4 = 0.24。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import operator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数说明:创建数据样本\n",
    "Returns:\n",
    "    dataset - 实验样本切分的词条\n",
    "    classVec - 类别标签向量\n",
    "\"\"\"\n",
    "def loadDataSet():\n",
    "    dataset = [ ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],    # 切分的词条\n",
    "                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid'] ]\n",
    "    classVec = [0, 1, 0, 1, 0, 1]  # 类别标签向量，1代表好，0代表不好\n",
    "    return dataset, classVec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "函数说明：特征选择TF-IDF算法\n",
    "Parameters:\n",
    "     list_words:词列表\n",
    "Returns:\n",
    "     dict_feature_select:特征选择词字典\n",
    "\"\"\"\n",
    "\n",
    "def feature_select(list_words):\n",
    "    #总词频统计\n",
    "    doc_frequence = defaultdict(int)\n",
    "    for doc in list_words:\n",
    "        for word in doc:\n",
    "            doc_frequence[word]+=1\n",
    "\n",
    "    #存储每个词的TF值\n",
    "    word_tf = {}\n",
    "    for word in doc_frequence:\n",
    "        word_tf[word] = doc_frequence[word]/sum(doc_frequence.values())\n",
    "\n",
    "    #计算每个词的IDF值\n",
    "    word_idf = {}\n",
    "    doc_len = len(list_words)\n",
    "    word_doc = defaultdict(int) #包含该词的文档数\n",
    "    for word in doc_frequence:\n",
    "        for doc in list_words:\n",
    "            if word in doc:\n",
    "                word_doc[word] += 1\n",
    "    for word in doc_frequence:\n",
    "        word_idf[word] = math.log(doc_len/(word_doc[word]+1))\n",
    "\n",
    "    #计算每个单词的tf-idf值\n",
    "    word_tf_idf = {}\n",
    "    for word in doc_frequence:\n",
    "        word_tf_idf[word] = word_tf[word] * word_idf[word]\n",
    "\n",
    "    # 对字典按值由大到小排序\n",
    "    dict_feature_select=sorted(word_tf_idf.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return dict_feature_select"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(int, {})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_frequence2 = defaultdict(int)\n",
    "doc_frequence2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 0.0322394037469742), ('stop', 0.0322394037469742), ('worthless', 0.0322394037469742)]\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "data_list, label_list = loadDataSet() #加载数据\n",
    "features = feature_select(data_list) #所有词的TF-IDF值\n",
    "print(features[0:3])\n",
    "print(len(features))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}